{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from warnings import simplefilter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import random\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "train_positive_path = '../Sentiment_Analysis/aclImdb/train/pos/'\n",
    "train_negative_path = '../Sentiment_Analysis/aclImdb/train/neg/'\n",
    "test_positive_path = '../Sentiment_Analysis/aclImdb/test/pos/'\n",
    "test_negative_path = '../Sentiment_Analysis/aclImdb/test/neg/'\n",
    "\n",
    "def read_files_in_path(path):\n",
    "    positive_training_data=[]\n",
    "    for filepath in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with open(filepath,encoding='utf8') as f:\n",
    "            content = f.read()\n",
    "            positive_training_data.append(content)\n",
    "    return positive_training_data\n",
    "\n",
    "#Read training data\n",
    "positive_training_data = read_files_in_path(train_positive_path)\n",
    "negative_training_data = read_files_in_path(train_negative_path)\n",
    "whole_training_data = positive_training_data+negative_training_data\n",
    "\n",
    "#Reading Test Data\n",
    "positive_test_data = read_files_in_path(test_positive_path)\n",
    "negative_test_data = read_files_in_path(test_negative_path)\n",
    "whole_test_data = positive_test_data+negative_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove HTML tags from dataset\n",
    "def cleanhtml(whole_training_data):\n",
    "    no_HTML_whole_training_data = []\n",
    "    cleaner = re.compile('<.*?>')\n",
    "    for review in whole_training_data:\n",
    "        cleantext = re.sub(cleaner, '', review)\n",
    "        no_HTML_whole_training_data.append(cleantext)\n",
    "    return no_HTML_whole_training_data\n",
    "\n",
    "#Remove stop words, punctuation and Lemmatize words\n",
    "def remove_punc_stop_words(no_HTML_whole_training_data):\n",
    "    clean_training_data = []\n",
    "    for review in no_HTML_whole_training_data:\n",
    "        #remove punctuation marks\n",
    "        review = review.lower()\n",
    "        for ch in string.punctuation:\n",
    "            review = review.replace(ch,' ')\n",
    "        #Remove stop words\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        word_tokens = word_tokenize(review) \n",
    "        filtered_words = [w for w in word_tokens if not w in stop_words] \n",
    "        #construct the review again\n",
    "        sentence =''\n",
    "        for word in filtered_words:\n",
    "            sentence+=lemmatize_word(word)\n",
    "            sentence+=' '\n",
    "        clean_training_data.append(sentence)\n",
    "    return clean_training_data\n",
    "\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "\n",
    "#clean data from HTML tags\n",
    "no_HTML_whole_training_data = cleanhtml(whole_training_data)\n",
    "# print(whole_training_data[210])\n",
    "# print('\\n',no_HTML_whole_training_data[210])\n",
    "\n",
    "clean_training_data = remove_punc_stop_words(no_HTML_whole_training_data)\n",
    "# print(set(stopwords.words('english')))\n",
    "# print('\\n',no_HTML_whole_training_data[210])\n",
    "# print('\\n',clean_training_data[210])\n",
    "# print('\\n',no_HTML_whole_training_data[310])\n",
    "# print('\\n',clean_training_data[310])\n",
    "# print('\\n',no_HTML_whole_training_data[4510])\n",
    "# print('\\n',clean_training_data[4510])\n",
    "no_HTML_whole_test_data = cleanhtml(whole_test_data)\n",
    "clean_test_data = remove_punc_stop_words(no_HTML_whole_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classification accuracy:\n",
      " 88.352 %\n"
     ]
    }
   ],
   "source": [
    "#create labels for the training data and test data first 12500 are positive and the remaining 12500 are negative\n",
    "train_labels = np.asarray([1]*len(positive_training_data) + [0]*len(negative_training_data))\n",
    "test_labels = np.asarray([1]*len(positive_test_data) + [0]*len(negative_test_data))\n",
    "\n",
    "#randomize test data\n",
    "z = list(zip(clean_training_data, train_labels))\n",
    "random.shuffle(z)\n",
    "random_clean_training_data, random_train_labels = zip(*z)\n",
    "\n",
    "#Apply feature extraction: countVectorizer which is based on bag of words algorithm\n",
    "vectorizer = CountVectorizer(ngram_range=(1,4))\n",
    "train_vector = vectorizer.fit_transform(random_clean_training_data)\n",
    "test_vector = vectorizer.transform(clean_test_data)\n",
    "#printing some visualization of the sizes of the train and test vectors \n",
    "#print(train_labels.shape)\n",
    "#train_vector has 25000 row corresponding to the 25000 review and 74849 feature extracted\n",
    "#print(train_vector.shape)\n",
    "\n",
    "#print shapes of test vector and label\n",
    "\n",
    "# print(test_labels.shape)\n",
    "# print(test_vector.shape)\n",
    "\n",
    "#some visualization for th labels arrays\n",
    "# print(random_train_labels)\n",
    "# print(test_labels)\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(train_vector, random_train_labels)\n",
    "print(\"Logistic Regression classification accuracy:\\n\",logreg.score(test_vector,test_labels)*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
