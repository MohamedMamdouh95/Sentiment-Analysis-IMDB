{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MohamedTourab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MohamedTourab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MohamedTourab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import random\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "train_positive_path = '../Sentiment_Analysis/aclImdb/train/pos/'\n",
    "train_negative_path = '../Sentiment_Analysis/aclImdb/train/neg/'\n",
    "test_positive_path = '../Sentiment_Analysis/aclImdb/test/pos/'\n",
    "test_negative_path = '../Sentiment_Analysis/aclImdb/test/neg/'\n",
    "\n",
    "def read_files_in_path(path):\n",
    "    positive_training_data=[]\n",
    "    for filepath in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with open(filepath,encoding='utf8') as f:\n",
    "            content = f.read()\n",
    "            positive_training_data.append(content)\n",
    "    return positive_training_data\n",
    "\n",
    "#Read training data\n",
    "positive_training_data = read_files_in_path(train_positive_path)\n",
    "negative_training_data = read_files_in_path(train_negative_path)\n",
    "whole_training_data = positive_training_data+negative_training_data\n",
    "\n",
    "#Reading Test Data\n",
    "positive_test_data = read_files_in_path(test_positive_path)\n",
    "negative_test_data = read_files_in_path(test_negative_path)\n",
    "whole_test_data = positive_test_data+negative_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove HTML tags from dataset\n",
    "def cleanhtml(whole_training_data):\n",
    "    no_HTML_whole_training_data = []\n",
    "    cleaner = re.compile('<.*?>')\n",
    "    for review in whole_training_data:\n",
    "        cleantext = re.sub(cleaner, '', review)\n",
    "        no_HTML_whole_training_data.append(cleantext)\n",
    "    return no_HTML_whole_training_data\n",
    "\n",
    "#Remove stop words, punctuation and Lemmatize words\n",
    "def remove_punc_stop_words(no_HTML_whole_training_data):\n",
    "    clean_training_data = []\n",
    "    for review in no_HTML_whole_training_data:\n",
    "        #remove punctuation marks\n",
    "        review = review.lower()\n",
    "        for ch in string.punctuation:\n",
    "            review = review.replace(ch,' ')\n",
    "        #Remove stop words\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        word_tokens = word_tokenize(review) \n",
    "        filtered_words = [w for w in word_tokens if not w in stop_words] \n",
    "        #construct the review again\n",
    "        sentence =''\n",
    "        for word in filtered_words:\n",
    "            sentence+=lemmatize_word(word)\n",
    "            sentence+=' '\n",
    "        clean_training_data.append(sentence)\n",
    "    return clean_training_data\n",
    "\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "\n",
    "#clean data from HTML tags\n",
    "no_HTML_whole_training_data = cleanhtml(whole_training_data)\n",
    "# print(whole_training_data[210])\n",
    "# print('\\n',no_HTML_whole_training_data[210])\n",
    "\n",
    "clean_training_data = remove_punc_stop_words(no_HTML_whole_training_data)\n",
    "# print(set(stopwords.words('english')))\n",
    "# print('\\n',no_HTML_whole_training_data[210])\n",
    "# print('\\n',clean_training_data[210])\n",
    "# print('\\n',no_HTML_whole_training_data[310])\n",
    "# print('\\n',clean_training_data[310])\n",
    "# print('\\n',no_HTML_whole_training_data[4510])\n",
    "# print('\\n',clean_training_data[4510])\n",
    "no_HTML_whole_test_data = cleanhtml(whole_test_data)\n",
    "clean_test_data = remove_punc_stop_words(no_HTML_whole_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create labels for the training data and test data first 12500 are positive and the remaining 12500 are negative\n",
    "train_labels = np.asarray([1]*len(positive_training_data) + [0]*len(negative_training_data))\n",
    "test_labels = np.asarray([1]*len(positive_test_data) + [0]*len(negative_test_data))\n",
    "\n",
    "#randomize test data\n",
    "z = list(zip(clean_training_data, train_labels))\n",
    "random.shuffle(z)\n",
    "random_clean_training_data, random_train_labels = zip(*z)\n",
    "\n",
    "#Apply feature extraction: Term Frequency inverse document frequency vectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools\n",
    "vectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1,2))\n",
    "train_vector = vectorizer.fit_transform(random_clean_training_data)\n",
    "test_vector = vectorizer.transform(clean_test_data)\n",
    "#printing some visualization of the sizes of the train and test vectors \n",
    "#print(train_labels.shape)\n",
    "#train_vector has 25000 row corresponding to the 25000 review and 74849 feature extracted\n",
    "#print(train_vector.shape)\n",
    "\n",
    "#print shapes of test vector and label\n",
    "\n",
    "# print(test_labels.shape)\n",
    "# print(test_vector.shape)\n",
    "\n",
    "#some visualization for th labels arrays\n",
    "# print(random_train_labels)\n",
    "# print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param_C  mean_test_score\n",
      "0     0.1          0.84052\n",
      "1       1          0.87868\n",
      "2      10          0.89168\n",
      "3      50          0.89472\n",
      "4     100          0.89504\n",
      "5    1000          0.89572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "\n",
    "clf = GridSearchCV(linear_model.LogisticRegression(),{'C':[0.1,1,10,50,100,1000]},cv=5,return_train_score=False)\n",
    "clf.fit(train_vector, random_train_labels)\n",
    "df=pd.DataFrame(clf.cv_results_)\n",
    "print(df[['param_C','mean_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89.72 89.42 88.5  90.18 89.7 ]\n",
      "Mean Accuracy: 89.50  (+/- 0.01 deviation) \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression(C=100)\n",
    "scores = cross_val_score(clf, train_vector, random_train_labels, cv=5)\n",
    "print(scores*100)\n",
    "print(\"Mean Accuracy: %0.2f  (+/- %0.2f deviation) \" % (scores.mean()*100, scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classification accuracy:\n",
      " 88.276 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(C=100)\n",
    "logreg.fit(train_vector, random_train_labels)\n",
    "print(\"Logistic Regression classification accuracy:\\n\",logreg.score(test_vector,test_labels)*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
